"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[5340],{82566:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var t=i(85893),o=i(11151);const s={sidebar_position:3},r="Video",a={id:"use_cases/vector_search/video_search",title:"Video",description:"Search Within Videos with Text",source:"@site/content/use_cases/vector_search/video_search.md",sourceDirName:"use_cases/vector_search",slug:"/use_cases/vector_search/video_search",permalink:"/docs/use_cases/vector_search/video_search",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/use_cases/vector_search/video_search.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Image",permalink:"/docs/use_cases/vector_search/multimodal_image_search_clip"},next:{title:"Chunked",permalink:"/docs/use_cases/vector_search/chunked_vector_search"}},d={},c=[{value:"Search Within Videos with Text",id:"search-within-videos-with-text",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Register Encoders",id:"register-encoders",level:2},{value:"Create CLIP Model",id:"create-clip-model",level:2},{value:"Create VectorIndex",id:"create-vectorindex",level:2},{value:"Query Text Against Saved Frames",id:"query-text-against-saved-frames",level:2},{value:"Start the Video from the Resultant Timestamp",id:"start-the-video-from-the-resultant-timestamp",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"video",children:"Video"}),"\n",(0,t.jsx)(n.h2,{id:"search-within-videos-with-text",children:"Search Within Videos with Text"}),"\n",(0,t.jsx)(n.p,{children:"This notebook guides you through the process of searching for specific textual information within videos and retrieving relevant video segments. To achieve this, we leverage various libraries and techniques, including:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"clip: A library for vision and language understanding."}),"\n",(0,t.jsx)(n.li,{children:"PIL: Python Imaging Library for image processing."}),"\n",(0,t.jsx)(n.li,{children:"torch: The PyTorch library for deep learning."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Searching within videos with text has practical applications in various domains:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Video Indexing:"})," People can find specific topics within videos, enhancing search experiences."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Content Moderation:"})," Social media platforms use text-based searches to identify and moderate content violations."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Content Discovery:"})," Users search for specific scenes or moments within movies or TV shows using text queries. Security personnel can search within video footage for specific incidents or individuals."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Your imagination is your limit. Basically, all this example is doing is making the video like a blog post and searchable as well!"}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"!pip install superduperdb\n!pip install ipython opencv-python pillow openai-clip\n"})}),"\n",(0,t.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,t.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For testing (default connection): ",(0,t.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,t.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,t.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,t.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,t.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import superduper, Collection, CFG\nimport os\n\n# Set configuration options for downloads\nCFG.downloads.hybrid = True\nCFG.downloads.root = './'\n\n# Define the MongoDB URI, with a default value if not provided\nmongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\n\n# SuperDuperDB, now handles your MongoDB database\n# It just super dupers your database by initializing a SuperDuperDB datalayer instance with a MongoDB backend and filesystem-based artifact store\ndb = superduper(mongodb_uri, artifact_store='filesystem://./data/')\n\n# Create a collection named 'videos'\nvideo_collection = Collection('videos')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,t.jsx)(n.p,{children:"We'll begin by configuring a video encoder."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Encoder\n\n# Create an instance of the Encoder with the identifier 'video_on_file' and load_hybrid set to False\nvid_enc = Encoder(\n    identifier='video_on_file',\n    load_hybrid=False,\n)\n\n# Add the Encoder instance to the SuperDuperDB instance\ndb.add(vid_enc)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Let's fetch a sample video from the internet and insert it into our collection."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb.base.document import Document\n\n# Insert a video document into the 'videos' collection\ndb.execute(\n    video_collection.insert_one(\n        Document({'video': vid_enc(uri='https://superduperdb-public.s3.eu-west-1.amazonaws.com/animals_excerpt.mp4')}) # Encodes the video\n    )\n)\n\n# Display the list of videos in the 'videos' collection\nlist(db.execute(Collection('videos').find()))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"register-encoders",children:"Register Encoders"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's set up encoders to process videos and extract frames. These encoders will assist in converting videos into individual frames."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import cv2\nimport tqdm\nfrom PIL import Image\nfrom superduperdb.ext.pillow import pil_image\nfrom superduperdb import Model, Schema\n\n# Define a function to convert a video file into a list of images\ndef video2images(video_file):\n    # Set the sampling frequency for frames\n    sample_freq = 10\n    \n    # Open the video file using OpenCV\n    cap = cv2.VideoCapture(video_file)\n    \n    # Initialize variables\n    frame_count = 0\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    extracted_frames = []\n    progress = tqdm.tqdm()\n\n    # Iterate through video frames\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Get the current timestamp based on frame count and FPS\n        current_timestamp = frame_count // fps\n        \n        # Sample frames based on the specified frequency\n        if frame_count % sample_freq == 0:\n            extracted_frames.append({\n                'image': Image.fromarray(frame[:,:,::-1]),  # Convert BGR to RGB\n                'current_timestamp': current_timestamp,\n            })\n        frame_count += 1\n        progress.update(1)\n    \n    # Release resources\n    cap.release()\n    cv2.destroyAllWindows()\n    \n    # Return the list of extracted frames\n    return extracted_frames\n\n# Create a SuperDuperDB model for the video2images function\nvideo2images_model = Model(\n    identifier='video2images',\n    object=video2images,\n    flatten=True,\n    model_update_kwargs={'document_embedded': False},\n    output_schema=Schema(identifier='myschema', fields={'image': pil_image})\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Additionally, we'll configure a listener to continually download video URLs and store the best frames in another collection."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Listener\n\n# Add a listener to process videos using the video2images model\ndb.add(\n   Listener(\n       model=video2images,  # Assuming video2images is your SuperDuperDB model\n       select=video_collection.find(),\n       key='video',\n   )\n)\n\n# Get the unpacked outputs of the video2images process for a specific video\noutputs = db.execute(Collection('_outputs.video.video2images').find_one()).unpack()\n\n# Display the image output from the processed video\nimage_output = outputs['_outputs']['video']['video2images']['image']\n"})}),"\n",(0,t.jsx)(n.h2,{id:"create-clip-model",children:"Create CLIP Model"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's establish a model for CLIP (Contrastive Language-Image Pre-training), serving for both visual and textual analysis."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\n# Load the CLIP model and define a tensor type\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\nt = vector(shape=(1024,))\n\n# Create a TorchModel for visual encoding\nvisual_model = TorchModel(\n    identifier='clip_image',\n    preprocess=preprocess,\n    object=model.visual,\n    encoder=t,\n    postprocess=lambda x: x.tolist(),\n)\n\n# Create a TorchModel for text encoding\ntext_model = TorchModel(\n    identifier='clip_text',\n    object=model,\n    preprocess=lambda x: clip.tokenize(x)[0],\n    forward_method='encode_text',\n    encoder=t,\n    device='cpu',  # Specify the device for text encoding\n    preferred_devices=None,  # Specify preferred devices for model execution\n    postprocess=lambda x: x.tolist(),\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"create-vectorindex",children:"Create VectorIndex"}),"\n",(0,t.jsx)(n.p,{children:"We'll now establish a VectorIndex to index and search the video frames based on both visual and textual content. This includes creating an indexing listener for visual data and a compatible listener for textual data."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from\n\n superduperdb import Listener, VectorIndex\nfrom superduperdb.backends.mongodb import Collection\n\n# Add a VectorIndex for video search\ndb.add(\n    VectorIndex(\n        identifier='video_search_index',\n        indexing_listener=Listener(\n            model=visual_model, # Visual model for image processing\n            key='_outputs.video.video2images.image', # Visual model for image processing\n            select=Collection('_outputs.video.video2images').find(), # Collection containing video image data\n        ),\n        compatible_listener=Listener(\n            model=text_model,  # Text model for processing associated text data\n            key='text',\n            select=None,\n            active=False\n        )\n    )\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"query-text-against-saved-frames",children:"Query Text Against Saved Frames"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's search for something that happened during the video:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define the search parameters\nsearch_term = 'Some ducks'\nnum_results = 1\n\n# Execute the search and get the next result\nr = next(db.execute(\n    Collection('_outputs.video.video2images')\n    .like(Document({'text': search_term}), vector_index='video_search_index', n=num_results)\n    .find()\n))\n\n# Extract the timestamp from the search result\nsearch_timestamp = r['_outputs']['video']['video2images']['current_timestamp']\n\n# Retrieve the back reference to the original video using the '_source' field\nvideo = db.execute(Collection('videos').find_one({'_id': r['_source']}))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"start-the-video-from-the-resultant-timestamp",children:"Start the Video from the Resultant Timestamp"}),"\n",(0,t.jsx)(n.p,{children:"Finally, we can display and play the video starting from the timestamp where the searched text is found."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from IPython.display import display, HTML\n\n# Create HTML code for the video player with a specified source and controls\nvideo_html = f"""\n<video width="640" height="480" controls>\n    <source src="{video[\'video\'].uri}" type="video/mp4">\n</video>\n<script>\n    // Get the video element\n    var video = document.querySelector(\'video\');\n    \n    // Set the current time of the video to the specified timestamp\n    video.currentTime = {search_timestamp};\n    \n    // Play the video automatically\n    video.play();\n<\/script>\n"""\n\n# Display the HTML code in the notebook\ndisplay(HTML(video_html))\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>r});var t=i(67294);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);