"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[5666],{65170:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>d});var t=o(85893),i=o(11151);const s={},r="Building Voice-Memo Assistant on MongoDB",a={id:"use_cases/question-answering/voice_memos",title:"Building Voice-Memo Assistant on MongoDB",description:"Cataloguing voice-memos for a self managed personal assistant",source:"@site/content/use_cases/question-answering/voice_memos.md",sourceDirName:"use_cases/question-answering",slug:"/use_cases/question-answering/voice_memos",permalink:"/docs/use_cases/question-answering/voice_memos",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/use_cases/question-answering/voice_memos.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Chatting with your SnowFlakes Database Using OpenAI",permalink:"/docs/use_cases/question-answering/chat_with_your_database"},next:{title:"Classical ML Applications",permalink:"/docs/category/classical-ml-applications"}},c={},d=[{value:"Cataloguing voice-memos for a self managed personal assistant",id:"cataloguing-voice-memos-for-a-self-managed-personal-assistant",level:2},{value:"Objectives:",id:"objectives",level:3},{value:"Our approach involves:",id:"our-approach-involves",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Install Pre-Trained Model (LibriSpeech) into Database",id:"install-pre-trained-model-librispeech-into-database",level:2},{value:"Run Predictions on All Recordings in the Collection",id:"run-predictions-on-all-recordings-in-the-collection",level:2},{value:"Ask Questions to Your Voice Assistant",id:"ask-questions-to-your-voice-assistant",level:2},{value:"Enrich with Chat-Completion",id:"enrich-with-chat-completion",level:2},{value:"Full Voice-Assistant Experience",id:"full-voice-assistant-experience",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"building-voice-memo-assistant-on-mongodb",children:"Building Voice-Memo Assistant on MongoDB"}),"\n",(0,t.jsx)(n.h2,{id:"cataloguing-voice-memos-for-a-self-managed-personal-assistant",children:"Cataloguing voice-memos for a self managed personal assistant"}),"\n",(0,t.jsx)(n.p,{children:"Explore the capabilities of SuperDuperDB by effortlessly integrating models across various data modalities, including audio and text. This project aims to develop sophisticated data-based applications with minimal code complexity."}),"\n",(0,t.jsx)(n.h3,{id:"objectives",children:"Objectives:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Manage a database of audio recordings."}),"\n",(0,t.jsx)(n.li,{children:"Index the content of these audio recordings."}),"\n",(0,t.jsx)(n.li,{children:"Perform searches and queries on the content of these audio recordings."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"our-approach-involves",children:"Our approach involves:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Using a transformers model from Facebook's AI team for audio-to-text transcription."}),"\n",(0,t.jsx)(n.li,{children:"Applying an OpenAI vectorization model to index the transcribed text."}),"\n",(0,t.jsx)(n.li,{children:"Combining the OpenAI ChatGPT model with relevant recordings to query the audio database."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Real-life use cases encompass personal note-taking, voice diaries, meeting transcriptions, language learning, task reminders, podcast indexing, knowledge base creation, journalism interviews, storytelling archives, and music catalog searches."}),"\n",(0,t.jsx)(n.p,{children:"In this example, we'll organize and catalog voice memos for a self-managed personal assistant using SuperDuperDB."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before diving into the implementation, ensure that you have the necessary libraries installed by running the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"!pip install superduperdb\n!pip install transformers soundfile torchaudio librosa openai\n!pip install -U datasets\n"})}),"\n",(0,t.jsx)(n.p,{children:"Additionally, ensure that you have set your openai API key as an environment variable. You can uncomment the following code and add your API key:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\n\n#os.environ['OPENAI_API_KEY'] = 'sk-XXXX'\nif 'OPENAI_API_KEY' not in os.environ:\n    raise Exception('Environment variable \"OPENAI_API_KEY\" not set')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,t.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup. Here are some examples of MongoDB URIs:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For testing (default connection): ",(0,t.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,t.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,t.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,t.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,t.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\nimport os\n\nmongodb_uri = os.getenv("MONGODB_URI","mongomock://test")\n\n# Superdupers your database\ndb = superduper(mongodb_uri)\n\n# Create a collection for Voice memos\nvoice_collection = Collection(\'voice-memos\')\n'})}),"\n",(0,t.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,t.jsxs)(n.p,{children:["In this example, we use the ",(0,t.jsx)(n.code,{children:"LibriSpeech"})," dataset as our voice recording dataset, containing around 1000 hours of read English speech. Similar functionality can be achieved with any audio source, including audio hosted on the web or in an ",(0,t.jsx)(n.code,{children:"s3"})," bucket. For instance, repositories of audio from conference calls or memos can be indexed in the same way."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from datasets import load_dataset\nfrom superduperdb.ext.numpy import array\nfrom superduperdb import Document\n\n# Load the LibriSpeech ASR demo data from Hugging Face datasets\ndata = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n\n# Create an `Encoder` for audio data\nenc = array('float64', shape=(None,))\n\n# Add the encoder to the SuperDuperDB instance\ndb.add(enc)\n\n# Insert audio data into the MongoDB collection 'voice_collection'\ndb.execute(voice_collection.insert_many([\n    # Create a SuperDuperDB Document for each audio sample\n    Document({'audio': enc(r['audio']['array'])}) for r in data\n]))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"install-pre-trained-model-librispeech-into-database",children:"Install Pre-Trained Model (LibriSpeech) into Database"}),"\n",(0,t.jsxs)(n.p,{children:["Apply a pre-trained ",(0,t.jsx)(n.code,{children:"transformers"})," model to the data:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\nfrom superduperdb.ext.transformers import Pipeline\n\n# Load the pre-trained Speech2Text model and processor from Facebook's library\nmodel = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\nprocessor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n\n# Define the sampling rate for the audio data\nSAMPLING_RATE = 16000\n\n# Create a SuperDuperDB pipeline for speech-to-text transcription\ntranscriber = Pipeline(\n    identifier='transcription',\n    object=model,  # The pre-trained Speech2Text model\n    preprocess=processor,  # The processor for handling input audio data\n    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True},  # Preprocessing configurations\n    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),  # Postprocessing to convert model output to text\n    predict_method='generate',  # Specify the prediction method\n    preprocess_type='other',  # Specify the type of preprocessing\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"run-predictions-on-all-recordings-in-the-collection",children:"Run Predictions on All Recordings in the Collection"}),"\n",(0,t.jsxs)(n.p,{children:["Apply the ",(0,t.jsx)(n.code,{children:"Pipeline"})," to all audio recordings:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"transcriber.predict(\n    X='audio',  # Specify the input feature name as 'audio'\n    db=db,  # Provide the SuperDuperDB instance\n    select=voice_collection.find(),  # Specify the collection of audio data to transcribe\n    max_chunk_size=10  # Set the maximum chunk size for processing audio data\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"ask-questions-to-your-voice-assistant",children:"Ask Questions to Your Voice Assistant"}),"\n",(0,t.jsx)(n.p,{children:"Interact with your voice assistant by asking questions, leveraging the capabilities of MongoDB for vector-search and filtering rules:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import VectorIndex, Listener\nfrom superduperdb.ext.openai import OpenAIEmbedding\n\n# Create a VectorIndex with OpenAI embedding for audio transcriptions\ndb.add(\n    VectorIndex(\n        identifier='my-index',  # Set a unique identifier for the VectorIndex\n        indexing_listener=Listener(\n            model=OpenAIEmbedding(model='text-embedding-ada-002'),  # Use OpenAIEmbedding for audio transcriptions\n            key='_outputs.audio.transcription',  # Specify the key for indexing the transcriptions in the output\n            select=voice_collection.find(),  # Select the collection of audio data to index\n        ),\n    )\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:'Let\'s verify the functionality by searching for the term "royal cavern."'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Define the search parameters\nsearch_term = 'royal cavern'  # Set the search term for audio transcriptions\nnum_results = 2  # Set the number of desired search results\n\n# Execute a search query using the VectorIndex 'my-index'\n# Search for audio transcriptions similar to the specified search term\n# and retrieve the specified number of results\nsearch_results = list(\n    db.execute(\n        voice_collection.like(\n            {'_outputs.audio.transcription': search_term},\n            n=num_results,\n            vector_index='my-index',  # Use the 'my-index' VectorIndex for similarity search\n        ).find({}, {'_outputs.audio.transcription': 1})  # Retrieve only the 'transcription' field in the results\n    )\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"enrich-with-chat-completion",children:"Enrich with Chat-Completion"}),"\n",(0,t.jsxs)(n.p,{children:["Connect the previous steps with gpt-3.5.turbo, a chat-completion model on OpenAI. The goal is to enhance completions by seeding them with the most relevant audio recordings, determined by their textual transcriptions. Retrieve these transcriptions using the previously configured ",(0,t.jsx)(n.code,{children:"VectorIndex"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Import the OpenAIChatCompletion module from superduperdb.ext.openai\nfrom superduperdb.ext.openai import OpenAIChatCompletion\n\n# Create an instance of OpenAIChatCompletion with the GPT-3.5-turbo model\nchat = OpenAIChatCompletion(\n    model='gpt-3.5-turbo',\n    prompt=(\n        'Use the following facts to answer this question\\n'\n        '{context}\\n\\n'\n        'Here\\'s the question:\\n'\n    ),\n)\n\n# Add the OpenAIChatCompletion instance to the database\ndb.add(chat)\n\n# Display the details of the added model in the database\nprint(db.show('model'))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"full-voice-assistant-experience",children:"Full Voice-Assistant Experience"}),"\n",(0,t.jsx)(n.p,{children:"Evaluate the complete model by asking a question related to a specific fact mentioned in the audio recordings. The model will retrieve the most relevant recordings and utilize them to formulate its answer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Document\n\n# Define a question to ask the chat completion model\nquestion = 'Is anything really Greek?'\n\n# Use the db.predict method to get a response from the GPT-3.5-turbo model\nresponse = db.predict(\n    model_name='gpt-3.5-turbo',\n    \n    # Input the question to the chat completion model\n    input=question,\n    \n    # Select relevant context for the model from the SuperDuperDB collection of audio transcriptions\n    context_select=voice_collection.like(\n        Document({'_outputs.audio.transcription': question}), vector_index='my-index'\n    ).find(),\n    \n    # Specify the key in the context used by the model\n    context_key='_outputs.audio.transcription',\n)[0].content\n\n# Print the response obtained from the chat completion model\nprint(response)\n"})})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},11151:(e,n,o)=>{o.d(n,{Z:()=>a,a:()=>r});var t=o(67294);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);