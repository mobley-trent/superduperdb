"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[2389],{49882:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>d});var s=t(85893),i=t(11151);const o={sidebar_position:4},r="Chunked",a={id:"use_cases/vector_search/chunked_vector_search",title:"Chunked",description:"Implementing Chunked Vector Search with Multiple Inputs per Document",source:"@site/content/use_cases/vector_search/chunked_vector_search.md",sourceDirName:"use_cases/vector_search",slug:"/use_cases/vector_search/chunked_vector_search",permalink:"/docs/use_cases/vector_search/chunked_vector_search",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/use_cases/vector_search/chunked_vector_search.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Video",permalink:"/docs/use_cases/vector_search/video_search"},next:{title:"SQL examples",permalink:"/docs/category/sql-examples-1"}},c={},d=[{value:"Implementing Chunked Vector Search with Multiple Inputs per Document",id:"implementing-chunked-vector-search-with-multiple-inputs-per-document",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"chunked",children:"Chunked"}),"\n",(0,s.jsx)(n.h2,{id:"implementing-chunked-vector-search-with-multiple-inputs-per-document",children:"Implementing Chunked Vector Search with Multiple Inputs per Document"}),"\n",(0,s.jsx)(n.p,{children:"Let's find specific text within documents using vector-search. In this\nexample, we show how to do vector-search. But here, we want to go one\nstep further. Let's search for smaller pieces of text within larger\ndocuments. For instance, a developer may store entire documents but\nwants to find specific parts or references inside those documents."}),"\n",(0,s.jsxs)(n.p,{children:["Here we will show you an example with Wikipedia dataset. Implementing\nthis kind of search is usually more complex, but with ",(0,s.jsx)(n.code,{children:"superduperdb"}),",\nit's just one extra command."]}),"\n",(0,s.jsx)(n.p,{children:"Real-life use cases for the described problem of searching for specific\ntext within documents using vector-search with smaller text units\ninclude:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Legal Document Analysis:"})," Lawyers could store entire legal\ndocuments and search for specific clauses, references, or terms\nwithin those documents."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scientific Research Papers:"})," Researchers might want to find and\nextract specific information or references within scientific papers."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Code Search in Version Control Systems:"})," Developers could store\nentire code files and search for specific functions, classes, or\ncode snippets within those files."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Content Management Systems:"})," Content managers may store complete\narticles and search for specific paragraphs or keywords within those\narticles."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Customer Support Ticket Analysis:"})," Support teams might store\nentire support tickets and search for specific issues or resolutions\nmentioned within the tickets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In each of these scenarios, the ability to efficiently search for and\nretrieve smaller text units within larger documents can significantly\nenhance data analysis and retrieval capabilities."}),"\n",(0,s.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,s.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via\nSuperDuperDB. You can configure the ",(0,s.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific\nsetup."]}),"\n",(0,s.jsx)(n.p,{children:"Here are some examples of MongoDB URIs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["For testing (default connection): ",(0,s.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,s.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,s.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,s.jsxs)(n.li,{children:["MongoDB with authentication:\n",(0,s.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,s.jsxs)(n.li,{children:["MongoDB Atlas:\n",(0,s.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\nfrom superduperdb import superduper\n\nmongodb_uri = os.getenv("MONGODB_URI", "mongomock://test")\n\n# SuperDuperDB, now handles your MongoDB database\n# It just super dupers your database \ndb = superduper(mongodb_uri)\n'})}),"\n",(0,s.jsx)(n.p,{children:"To demonstrate this search technique with larger text units, we'll use a Wikipedia sample. Run this command to fetch the data."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Downloading the Wikipedia sample JSON file\n!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/wikipedia-sample.json\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Just like before, we insert the data using a syntax similar to\n",(0,s.jsx)(n.code,{children:"pymongo"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import json\nfrom superduperdb.backends.mongodb import Collection\nfrom superduperdb import Document as D\n\n# Read the first 100 records from a JSON file ('wikipedia-sample.json')\nwith open('wikipedia-sample.json') as f:\n    data = json.load(f)[:100]\n\n# Connect to the database and insert the data into the 'wikipedia' collection. 'D(r)' converts each record 'r' into a 'Document' object before insertion\ndb.execute(Collection('wikipedia').insert_many([D(r) for r in data]))\n"})}),"\n",(0,s.jsx)(n.p,{children:"Let's take a look at a document"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Executing a find_one query on the 'wikipedia' collection and unpacking the result\nr = db.execute(Collection('wikipedia').find_one()).unpack()\n\n# Displaying the result\nr\n"})}),"\n",(0,s.jsx)(n.p,{children:"To create the search functionality, we establish a straightforward model designed to break down the raw text into segments. These segments are then stored in another collection:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from superduperdb import Model\n\n# Define a function 'splitter' to split the 'abstract' field of a document into chunks.\ndef splitter(r):\n    # Initialize the output list with the document title\n    out = [r['title']]\n    # Split the 'abstract' field into chunks of 5 words\n    split = r['abstract'].split(' ')\n    # Iterate over the chunks and add them to the output list\n    for i in range(0, len(split) - 5, 5):\n        out.append(' '.join(split[i: i + 5]))\n    # Filter out empty strings from the output list\n    out = [x for x in out if x]\n    return out\n\n# Create a 'Model' instance named 'splitter' with the defined 'splitter' function\nmodel = Model(\n    identifier='splitter', # Identifier for the model\n    object=splitter, # The function to be used as a model\n    flatten=True, # Flatten the output into a single list\n    model_update_kwargs={'document_embedded': False}, # Model update arguments\n)\n\n# Use the 'predict' method of the model to get predictions for the input 'r'. one=true indicates that we only want one output to check!\nmodel.predict(r, one=True)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Let's utilize this model across the entire input collection:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Use the 'predict' method of the model\nmodel.predict(\n    X='_base', # Input data used by the model \n    db=db, # Database instance (assuming 'db' is defined earlier in your code)\n    select=Collection('wikipedia').find() # MongoDB query to select documents from the 'wikipedia' collection\n)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now let's look at the split data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Using the 'execute' method to execute a MongoDB query\n# Finding one document in the collection '_outputs._base.splitter'\ndb.execute(Collection('_outputs._base.splitter').find_one())\n"})}),"\n",(0,s.jsx)(n.p,{children:"We can perform a search on this data in a manner similar to the previous\nexample:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from superduperdb import VectorIndex, Listener\nfrom superduperdb.ext.openai import OpenAIEmbedding\n\n# Create an instance of the OpenAIEmbedding model with 'text-embedding-ada-002'\nmodel = OpenAIEmbedding(model='text-embedding-ada-002')\n\n\n# Add a VectorIndex to the database\ndb.add(\n    VectorIndex(\n        identifier=f'chunked-documents', # Identifier for the VectorIndex\n        indexing_listener=Listener(\n            model=model,  # Embedding model used for indexing\n            key='_outputs._base.splitter', # Key to access the embeddings in the database\n            select=Collection('_outputs._base.splitter').find(), # MongoDB query to select documents for indexing\n            predict_kwargs={'max_chunk_size': 1000}, # Additional parameters for the model's predict method like chunk size\n        ),\n        compatible_listener=Listener(\n            model=model, # Embedding model used for compatibility checking\n            key='_base', \n            select=None,  # No specific MongoDB query for Listener\n            active=False, \n        )\n    )\n)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now we can search through the split-text collection and retrieve the\nfull original documents, highlighting which text was found to be\nrelevant:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from superduperdb.backends.mongodb import Collection\nfrom superduperdb import Document as D\nfrom IPython.display import *\n\n# Define the query\nquery = 'politics'\n\n# Specify the shingle and main collections\nshingle_collection = Collection('_outputs._base.splitter')\nmain_collection = Collection('wikipedia')\n\n# Execute a search using superduperdb\nresult = db.execute(\n    shingle_collection\n        .like(D({'_base': query}), vector_index='chunked-documents', n=5)\n        .find({}, {'_outputs._base.text-embedding-ada-002': 0})\n)\n\n# Display the search results\ndisplay(Markdown(f'---'))\n\n# Iterate over the search results\nfor shingle in result:\n    # Retrieve the original document from the main collection\n    original = db.execute(main_collection.find_one({'_id': shingle['_source']}))\n    \n    # Display the title of the original document\n    display(Markdown(f'# {original[\"title\"]}\"'))\n    \n    # Highlight the shingle in the abstract of the original document\n    start = original['abstract'].find(shingle['_outputs']['_base']['splitter'])\n\n    to_format = (\n        original[\"abstract\"][:start] + '**' + '<span style=\"color:red\">' +\n        shingle[\"_outputs\"][\"_base\"][\"splitter\"].upper() + '**' + '<span style=\"color:black\">' +\n        original[\"abstract\"][start + len(shingle[\"_outputs\"][\"_base\"][\"splitter\"]):]\n    )\n    \n    # Display the formatted abstract\n    display(Markdown(to_format))\n    display(Markdown(f'---'))\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>r});var s=t(67294);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);