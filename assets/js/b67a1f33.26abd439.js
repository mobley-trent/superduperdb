"use strict";(self.webpackChunknewdocs=self.webpackChunknewdocs||[]).push([[8172],{76018:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var t=i(85893),s=i(11151);const o={sidebar_position:2},r="Building a Multimodal Vector-Search Using CLIP on MongoDB",a={id:"use_cases/vector_search/multimodal_image_search_clip",title:"Building a Multimodal Vector-Search Using CLIP on MongoDB",description:"This notebook demonstrates how SuperDuperDB can perform multimodal searches using the VectorIndex. It highlights SuperDuperDB's flexibility in integrating different models for vectorizing diverse queries during search and inference. In this example, we utilize the CLIP multimodal architecture.",source:"@site/content/use_cases/vector_search/multimodal_image_search_clip.md",sourceDirName:"use_cases/vector_search",slug:"/use_cases/vector_search/multimodal_image_search_clip",permalink:"/docs/use_cases/vector_search/multimodal_image_search_clip",draft:!1,unlisted:!1,editUrl:"https://github.com/SuperDuperDB/superduperdb/blob/main/docs/hr/content/use_cases/vector_search/multimodal_image_search_clip.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Building a Vanilla Text Vector-Search on MongoDB",permalink:"/docs/use_cases/vector_search/plain_vector_search"},next:{title:"Build in Video Vector-Search with Text on MongoDB",permalink:"/docs/use_cases/vector_search/video_search"}},c={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Connect to datastore",id:"connect-to-datastore",level:2},{value:"Load Dataset",id:"load-dataset",level:2},{value:"Build Models",id:"build-models",level:2},{value:"Create a Vector-Search Index",id:"create-a-vector-search-index",level:2},{value:"Search Images Using Text",id:"search-images-using-text",level:2},{value:"Now let&#39;s try Similarity search",id:"now-lets-try-similarity-search",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"building-a-multimodal-vector-search-using-clip-on-mongodb",children:"Building a Multimodal Vector-Search Using CLIP on MongoDB"}),"\n",(0,t.jsxs)(n.p,{children:["This notebook demonstrates how SuperDuperDB can perform multimodal searches using the ",(0,t.jsx)(n.code,{children:"VectorIndex"}),". It highlights SuperDuperDB's flexibility in integrating different models for vectorizing diverse queries during search and inference. In this example, we utilize the ",(0,t.jsx)(n.a,{href:"https://openai.com/research/clip",children:"CLIP multimodal architecture"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Real life use cases could be vectorizing diverse things like images and searching it efficiently."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting, make sure you have the required libraries installed. Run the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"!pip install superduperdb\n!pip install ipython openai-clip\n!pip install -U datasets\n"})}),"\n",(0,t.jsx)(n.h2,{id:"connect-to-datastore",children:"Connect to datastore"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the ",(0,t.jsx)(n.code,{children:"MongoDB_URI"})," based on your specific setup.\nHere are some examples of MongoDB URIs:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["For testing (default connection): ",(0,t.jsx)(n.code,{children:"mongomock://test"})]}),"\n",(0,t.jsxs)(n.li,{children:["Local MongoDB instance: ",(0,t.jsx)(n.code,{children:"mongodb://localhost:27017"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB with authentication: ",(0,t.jsx)(n.code,{children:"mongodb://superduper:superduper@mongodb:27017/documents"})]}),"\n",(0,t.jsxs)(n.li,{children:["MongoDB Atlas: ",(0,t.jsx)(n.code,{children:"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>"})]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom superduperdb import superduper\nfrom superduperdb.backends.mongodb import Collection\n\nmongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\ndb = superduper(mongodb_uri, artifact_store='filesystem://./models/')\n\n# SuperDuperDB, now handles your MongoDB database\n# It just super dupers your database \ndb = superduper(mongodb_uri, artifact_store='filesystem://.data')\n\ncollection = Collection('multimodal')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,t.jsxs)(n.p,{children:["For simplicity and interactivity, we'll use a subset of the ",(0,t.jsx)(n.a,{href:"https://paperswithcode.com/dataset/tiny-imagenet",children:"Tiny-Imagenet dataset"}),". The processes shown here can be applied to larger datasets with higher-resolution images. If working with larger datasets, especially with high-resolution images, it's recommended to use a machine with a GPU for efficiency."]}),"\n",(0,t.jsxs)(n.p,{children:["To insert images into the database, we'll use the ",(0,t.jsx)(n.code,{children:"Encoder"}),"-",(0,t.jsx)(n.code,{children:"Document"})," framework. This framework allows saving Python class instances as blobs in the ",(0,t.jsx)(n.code,{children:"Datalayer"})," and retrieving them as Python objects. SuperDuperDB comes with built-in support for ",(0,t.jsx)(n.code,{children:"PIL.Image"})," instances, making it easy to integrate Python AI models with the datalayer. If needed, you can also create custom encoders."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/coco_sample.zip\n!unzip coco_sample.zip\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import Document\nfrom superduperdb.ext.pillow import pil_image as i\nimport glob\nimport random\n\n# Use glob to get a list of image file paths in the 'images_small' directory\nimages = glob.glob('images_small/*.jpg')\n\n# Create a list of SuperDuperDB Document instances with image data\n# Note: The 'uri' parameter is set to the file URI using the 'file://' scheme\n# The list is limited to the first 500 images for demonstration purposes\ndocuments = [Document({'image': i(uri=f'file://{img}')}) for img in images][:500]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Access a random Document in the documents list, just to check:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Access a random Document in the documents list, just to check\ndocuments[1]\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The wrapped python dictionaries may be inserted directly to the ",(0,t.jsx)(n.code,{children:"Datalayer"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Insert the list of Document instances into a collection using SuperDuperDB\n# Specify the 'i' encoder (pil_image) for the 'image' field\ndb.execute(collection.insert_many(documents), encoders=(i,))\n"})}),"\n",(0,t.jsx)(n.p,{children:"You can verify that the images are correctly stored as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"x = db.execute(imagenet_collection.find_one()).unpack()['image']\n\n# Resize the image for display while maintaining the aspect ratio and Display the resized image\ndisplay(x.resize((300, 300 * int(x.size[1] / x.size[0]))))\n"})}),"\n",(0,t.jsx)(n.h2,{id:"build-models",children:"Build Models"}),"\n",(0,t.jsxs)(n.p,{children:["Now, let's prepare the CLIP model for multimodal search. This involves two components: ",(0,t.jsx)(n.code,{children:"text encoding"})," and ",(0,t.jsx)(n.code,{children:"visual encoding"}),". Once both components are installed, you can perform searches using both images and text to find matching items."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import clip\nfrom superduperdb import vector\nfrom superduperdb.ext.torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\n\n# Define a vector with shape (1024,)\ne = vector(shape=(1024,))\n\n# Create a TorchModel for text encoding\ntext_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    encoder=e,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nvisual_model = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    encoder=e, # Vector encoder with shape (1024,)\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"create-a-vector-search-index",children:"Create a Vector-Search Index"}),"\n",(0,t.jsxs)(n.p,{children:["Now, let's create the index for vector-based searching. We'll register both models with the index simultaneously. Specify that the ",(0,t.jsx)(n.code,{children:"visual_model"})," will be responsible for creating vectors in the database (",(0,t.jsx)(n.code,{children:"indexing_listener"}),"). The ",(0,t.jsx)(n.code,{children:"compatible_listener"})," indicates how an alternative model can be used to search the vectors, allowing multimodal search with models expecting different types of indexes."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from superduperdb import VectorIndex\nfrom superduperdb import Listener\n\n# Create a VectorIndex and add it to the database\ndb.add(\n    VectorIndex(\n        'my-index', # Unique identifier for the VectorIndex\n        indexing_listener=Listener(\n            model=visual_model, # Visual model for embeddings\n            key='image', # Key field in documents for embeddings\n            select=collection.find(), # Select the documents for indexing\n            predict_kwargs={'batch_size': 10}, # Prediction arguments for the indexing model\n        ),\n        compatible_listener=Listener(\n            # Create a listener to listen upcoming changes in databases\n            model=text_model, \n            key='text', \n            active=False, \n            select=None,\n        )\n    )\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"search-images-using-text",children:"Search Images Using Text"}),"\n",(0,t.jsx)(n.p,{children:"Now we can demonstrate searching for images using text queries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from IPython.display import display\nfrom superduperdb import Document\n\nquery_string = 'sports'\n\n# Execute the 'like' query using the VectorIndex 'my-index' and find the top 3 results\nout = db.execute(\n    collection.like(Document\n\n({'text': query_string}), vector_index='my-index', n=3).find({})\n)\n\n# Display the images from the search results\nfor r in search_results:\n    x = r['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n"})}),"\n",(0,t.jsx)(n.p,{children:"Let's dig further:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"img = db.execute(collection.find_one({}))['image']\nimg.x\n"})}),"\n",(0,t.jsx)(n.h2,{id:"now-lets-try-similarity-search",children:"Now let's try Similarity search"}),"\n",(0,t.jsx)(n.p,{children:"Perform a similarity search using the vector index 'my-index'\nFind the top 3 images similar to the input image 'img'\nFinally displaying the retrieved images while resizing them for better visualization."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Execute the 'like' query using the VectorIndex 'my-index' to find similar images to the specified 'img'\nsimilar_images = db.execute(\n    collection.like(Document({'image': img}), vector_index='my-index', n=3).find({})\n)\n\n# Display the similar images from the search results\nfor i in similar_images:\n    x = i['image'].x\n    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>r});var t=i(67294);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);